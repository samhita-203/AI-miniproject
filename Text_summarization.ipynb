{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_summarization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vkKGyVzWfc9",
        "outputId": "47aeb7ac-62b7-4985-ec1b-4accd2838030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "from heapq import nlargest"
      ],
      "metadata": {
        "id": "KGtEHDVAWqLf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnYpMFLBXFss",
        "outputId": "cabf7dda-9d24-4d89-8caa-fe6a34675df2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"As the hype around AI has accelerated, vendors have been scrambling to promote how their products and services use AI. Often what they refer to as AI is simply one component of AI, such as machine learning. AI requires a foundation of specialized hardware and software for writing and training machine learning algorithms. No one programming language is synonymous with AI, but a few, including Python, R and Java, are popular.\""
      ],
      "metadata": {
        "id": "fvSs2LReXJBl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if text.count(\". \") > 20:\n",
        "    length = int(round(text.count(\". \")/10, 0))\n",
        "else:\n",
        "    length = 1\n",
        "\n",
        "nopuch =[char for char in text if char not in string.punctuation]\n",
        "nopuch = \"\".join(nopuch)\n",
        "\n",
        "processed_text = [word for word in nopuch.split() if word.lower() not in nltk.corpus.stopwords.words('english')]\n",
        "\n",
        "word_freq = {}\n",
        "for word in processed_text:\n",
        "    if word not in word_freq:\n",
        "        word_freq[word] = 1\n",
        "    else:\n",
        "        word_freq[word] = word_freq[word] + 1\n",
        "\n",
        "max_freq = max(word_freq.values())\n",
        "for word in word_freq.keys():\n",
        "    word_freq[word] = (word_freq[word]/max_freq)\n",
        "\n",
        "sent_list = nltk.sent_tokenize(text)\n",
        "sent_score = {}\n",
        "for sent in sent_list:\n",
        "    for word in nltk.word_tokenize(sent.lower()):\n",
        "        if word in word_freq.keys():\n",
        "            if sent not in sent_score.keys():\n",
        "                sent_score[sent] = word_freq[word]\n",
        "            else:\n",
        "                sent_score[sent] = sent_score[sent] + word_freq[word]\n",
        "\n",
        "summary_sents = nlargest(length, sent_score, key=sent_score.get)\n",
        "summary = \" \".join(summary_sents)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrWeAknyXR12",
        "outputId": "e759222d-0e29-4ff5-8fad-4d64008e1c71"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI requires a foundation of specialized hardware and software for writing and training machine learning algorithms.\n"
          ]
        }
      ]
    }
  ]
}